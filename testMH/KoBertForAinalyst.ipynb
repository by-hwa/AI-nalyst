{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f17009-2df3-44f0-a327-33f4269360c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mxnet\n",
    "# !pip install gluonnlp\n",
    "# !pip install sentencepiece\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "# !pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b53d79-1b60-4b0e-be44-7879949cdf94",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdbd8c8-60d9-4142-a4d2-bed5d700b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gluonnlp as nlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback, AdamW\n",
    "\n",
    "# from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "# from kobert.utils import get_tokenizer\n",
    "# from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d1d84-7e7c-4d56-8853-7246ca8dd01b",
   "metadata": {},
   "source": [
    "### Device & Random_seed setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02576ef-cf8d-474e-abc6-6e247b974191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe2557-0b1c-4e1f-be8f-7e50770e717d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c12fa-20de-4a59-b9f7-4c353a915ee2",
   "metadata": {},
   "source": [
    "### Kobert for Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26dbcaef-69d8-462a-a1ed-d208bcfcab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'skt/kobert-base-v1'\n",
    "kobert = AutoModel.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1465c41-dc41-4cc7-a8ed-597a87bb950b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e22f3728-ef98-48b8-93bc-c6fecdd3bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e8f1ba7701d4e902\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-e8f1ba7701d4e902/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dfd6bd186a4b0fac125bef2bfe632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-21a9e1aedcee44a1\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-21a9e1aedcee44a1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856a824bc7d04e99a5a9c9b2c136376f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/piai/.cache/huggingface/datasets/csv/default-e8f1ba7701d4e902/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-f24d4418ea6a3ed7.arrow\n",
      "Loading cached processed dataset at /home/piai/.cache/huggingface/datasets/csv/default-21a9e1aedcee44a1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-37a56cd247a4152b.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "_metric = load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "def tokenized_fn(data):\n",
    "    outputs = tokenizer(data[\"article\"], padding=True, max_length=MAX_LEN, truncation=True)\n",
    "    if 'label' in data:\n",
    "        outputs[\"labels\"] = data[\"label\"]\n",
    "    return outputs\n",
    "\n",
    "def metric_fn(p):\n",
    "    preds, labels = p\n",
    "    output = _metric.compute(references=lables, predictions=np.argmax(preds, axis=-1))\n",
    "    return output\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"./data/train_report.csv\")[\"train\"]\n",
    "valid_dataset = load_dataset(\"csv\", data_files=\"./data/valid_report.csv\")[\"train\"]\n",
    "\n",
    "train_dataset = train_dataset.map(tokenized_fn, remove_columns=['filename', 'article', 'length', 'label'])\n",
    "valid_dataset = valid_dataset.map(tokenized_fn, remove_columns=['filename', 'article', 'length', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af1d2c3a-92fc-48a4-a4b6-5704f8bc51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.dense1 = torch.nn.Linear(768, 3072)\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "        # self.dense2 = torch.nn.Linear(3072, 768)\n",
    "        self.output = torch.nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # 보통 분류기에선 start 토큰에 분류 결과를 담음\n",
    "        x = features[:, 0, :]    # take <s> token (equiv. to [CLS])\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "        x = self.dropout(x)\n",
    "#         x = self.dense1(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class KobertForAinalyst(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(KobertForAinalyst, self).__init__()\n",
    "        self.model = model\n",
    "        self.classifier = ClassificationHead()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # labels=labels\n",
    "        )\n",
    "        self.labels = labels\n",
    "        logits = self.classifier(outputs[\"last_hidden_state\"])\n",
    "        # prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ca2388a-7e38-4fe9-a8d6-c4050844927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KobertForAinalyst(model=kobert)\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "isParallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ec99053-6d50-44c8-b69e-fed37c1f39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = torch.utils.data.RandomSampler(train_dataset),\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = _collator,\n",
    ")\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    sampler = torch.utils.data.SequentialSampler(valid_dataset),\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = _collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38c041aa-0c19-4e0a-95b6-489cee7b4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41ae679b-f90e-4528-9008-78397add1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df5195bc-d953-4b2f-8968-12b6054c0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 1/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:11<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.68620\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.55580\n",
      "\n",
      "==================== Epoch 2/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:15<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.67214\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.60268\n",
      "\n",
      "==================== Epoch 3/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:12<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.65747\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.54688\n",
      "\n",
      "==================== Epoch 4/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:11<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.57733\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.78571\n",
      "\n",
      "==================== Epoch 5/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:13<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.44837\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.83482\n",
      "\n",
      "==================== Epoch 6/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:13<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.38694\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.82812\n",
      "\n",
      "==================== Epoch 7/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:14<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.32772\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.86607\n",
      "\n",
      "==================== Epoch 8/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:15<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.30163\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.89062\n",
      "\n",
      "==================== Epoch 9/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:13<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.26507\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.92634\n",
      "\n",
      "==================== Epoch 10/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:14<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.23675\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.86161\n",
      "\n",
      "==================== Epoch 11/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:12<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.18758\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.94866\n",
      "\n",
      "==================== Epoch 12/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:12<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.16031\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.96875\n",
      "\n",
      "==================== Epoch 13/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:11<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.17031\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.95312\n",
      "\n",
      "==================== Epoch 14/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:11<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.11417\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.96429\n",
      "\n",
      "==================== Epoch 15/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:12<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.09854\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.98884\n",
      "\n",
      "==================== Epoch 16/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:12<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.07089\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.97991\n",
      "\n",
      "==================== Epoch 17/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:16<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.08388\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.97768\n",
      "\n",
      "==================== Epoch 18/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:12<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.06955\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.99554\n",
      "\n",
      "==================== Epoch 19/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:11<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.06379\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.99330\n",
      "\n",
      "==================== Epoch 20/20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 138/138 [01:11<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Average training loss: 0.06719\n",
      "\n",
      "Running Validation...\n",
      "    >>> Accuracy: 0.99107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clear_cache()\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f\"==================== Epoch {epoch+1}/{epochs} ====================\")\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training: \", ncols=100)):\n",
    "        batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        logits, loss = model(\n",
    "            input_ids = batch_input_ids,\n",
    "            attention_mask = batch_attention_mask,\n",
    "            labels = batch_labels,\n",
    "        )\n",
    "\n",
    "        if isParallel:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            print(\"step : {:>5,} of {:>5,} loss: {:.5f}\".format(step, len(train_dataloader), loss.item()))\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"    >>> Average training loss: {avg_train_loss:.5f}\", end='\\n\\n')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    print(\"Running Validation...\")\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(\n",
    "                input_ids = batch_input_ids,\n",
    "                attention_mask = batch_attention_mask,\n",
    "                labels = batch_labels,\n",
    "            )\n",
    "            \n",
    "            if isParallel:\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = batch_labels.to(\"cpu\").numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n",
    "    print(f\"    >>> Accuracy: {avg_val_accuracy:.5f}\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad18128e-c09c-462a-96af-15533a8ce911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cad80f5e825dbe52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/piai/.cache/huggingface/datasets/csv/default-cad80f5e825dbe52/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8fcdcfa7df4c25b83dbcc3c2cfc342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ee0c7277104dbd834096df3cbcc171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/piai/.cache/huggingface/datasets/csv/default-cad80f5e825dbe52/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17029761ce5f494797ee2a7e998a7b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab52bfda4434986a6cf358f96c45bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"csv\", data_files=f\"./data/test_report.csv\")[\"train\"]\n",
    "test_dataset = test_dataset.map(tokenized_fn, remove_columns=[\"filename\", \"article\",\"length\", \"label\"])\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    sampler = torch.utils.data.SequentialSampler(test_dataset),\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = _collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5b6c3e0-5648-403c-9bbd-13b5e143f63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Testing...\n",
      "    >>> Accuracy: 0.87143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run Testing...\")\n",
    "\n",
    "model.eval()\n",
    "total_test_accuracy = 0\n",
    "total_test_loss = 0\n",
    "nb_test_steps = 0\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "    batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    batch_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(\n",
    "            input_ids = batch_input_ids,\n",
    "            attention_mask = batch_attention_mask,\n",
    "            labels = batch_labels,\n",
    "        )\n",
    "\n",
    "        if isParallel:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        total_test_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch_labels.to(\"cpu\").numpy()\n",
    "        total_test_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "print(f\"    >>> Accuracy: {avg_test_accuracy:.5f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78834469-bbd9-4eed-81bc-876ffbae49bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d2549ac2fb5ac5b1\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-d2549ac2fb5ac5b1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2011744696e4f1f87607d41e9f76a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02384a95344846e8a0f9a72f99b2a8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50083 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_dataset = load_dataset(\"csv\", data_files=f\"./data/report_dataset.csv\")[\"train\"]\n",
    "inference_dataset = inference_dataset.map(tokenized_fn, remove_columns=[\"Unnamed: 0\", \"company\", \"title\", \"article\", \"opinion\", \"firm\", \"date\"])\n",
    "\n",
    "inference_dataloader = torch.utils.data.DataLoader(\n",
    "    inference_dataset,\n",
    "    sampler = torch.utils.data.SequentialSampler(inference_dataset),\n",
    "    batch_size = 1,\n",
    "    collate_fn = _collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bf75cdc-1440-4adc-ab71-0594d81d8d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference : 100%|█████████████████████████████████████████████| 50083/50083 [22:51<00:00, 36.53it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "probabilities = []\n",
    "predictions = []\n",
    "\n",
    "for step, batch in enumerate(tqdm(inference_dataloader, desc=\"Inference \", ncols=100)):\n",
    "    batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "    batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids = batch_input_ids,\n",
    "            attention_mask = batch_attention_mask,\n",
    "        )\n",
    "        \n",
    "        prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predict = torch.argmax(prob, axis=1)\n",
    "        \n",
    "        prob = np.trunc(np.max(prob.detach().cpu().numpy(), axis=1) * 100)\n",
    "        predict = predict.detach().cpu().numpy()\n",
    "        \n",
    "        probabilities.append(prob[0])\n",
    "        predictions.append(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dbc7bd22-8a84-4e15-8eac-b506c67a4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_inference = pd.read_csv(\"./data/report_dataset.csv\")\n",
    "\n",
    "convert_predictions = list(map(lambda x: \"매수\" if x == 1 else \"매도\", predictions))\n",
    "origin_inference = origin_inference.drop(labels=\"Unnamed: 0\", axis=1)\n",
    "origin_inference[\"predictions\"] = convert_predictions\n",
    "origin_inference[\"pred_rate\"] = probabilities\n",
    "origin_inference.to_csv(f\"./data/convert_inference_data_Kobert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c95bc-f04b-4153-95d5-5b92d0026cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = TrainingArguments(\n",
    "#     'models/',\n",
    "#     overwrite_output_dir=True,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=30,\n",
    "#     save_total_limit=5,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     # eval_epochs=1,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     load_best_model_at_end = True,\n",
    "#     # metric_for_best_model = 'f1',\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         data_collator=_collator,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=valid_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         compute_metrics=metric_fn,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience = 5)]\n",
    "# )\n",
    "\n",
    "# do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010186cc-6c03-4d51-bcde-c2f991c257e0",
   "metadata": {},
   "source": [
    "---\n",
    "#### Kobert by using kobert library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5357c5b4-c602-41ce-b470-853944958e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/piai/hustar/Hustar_Group_4_TeamP/testMH/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
      "/home/piai/hustar/Hustar_Group_4_TeamP/testMH/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "model, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f705d1-b6f4-4b45-abbb-dc1c1a481103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nlp.data.TSVDataset('./data/train_report.tsv', field_indices=[1,3], num_discard_samples=1)\n",
    "valid_dataset = nlp.data.TSVDataset('./data/valid_report.tsv', field_indices=[1,3], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89994ad0-753c-457c-a1fc-0ce703e28653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하지만 이번 경기 Cycle 에는 이자유예, 양극화 등 여러 요인으로 자산건전성이 경기에 후행할 수도 있다. 2021 년 순이익 추정치를 1 조 5,560 억원으로 6% 상향조정하지만, 여느 때에 비해 실적의 불확실성은 높은 편이다. 장기적인 배당 불확실성 감안하여 목표주가 9,000 원으로 하향조정기업은행은 국책은행이어서 금감원의 배당 축소 권고 대상에 포함되지 않는다. 그럼에도 불구하고 과연 배당성향을 더 높일 수 있을지는 불확실하다. 앞으로도 정책적인 역할을 중시할 가능성이 있기 때문이다. 장기 배당성향 기대치를 35%에서 30%로 낮추면서목표주가를 10,000 원에서 9,000 원으로 하향조정한다. 기존의 ‘중립’ 의견을 유지한다.',\n",
       " '0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f609ae8f-dffb-41dc-b394-9921e43fba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf201881-f41e-4c75-8ca1-ef02831bf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 30\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91cd97ed-5dc0-4342-b6a8-036197275f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #토큰화\n",
    "# tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)\n",
    "\n",
    "# #BERTDataset 클래스 이용, TensorDataset으로 만들어주기\n",
    "# train_dataset = BERTDataset(train_dataset, 0, 1, tokenizer, max_len, True, False)\n",
    "# valid_dataset = BERTDataset(valid_dataset, 0, 1, tokenizer, max_len, True, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
