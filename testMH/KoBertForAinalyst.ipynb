{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f17009-2df3-44f0-a327-33f4269360c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mxnet\n",
    "# !pip install gluonnlp\n",
    "# !pip install sentencepiece\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "# !pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b53d79-1b60-4b0e-be44-7879949cdf94",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdbd8c8-60d9-4142-a4d2-bed5d700b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gluonnlp as nlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback, AdamW\n",
    "\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d1d84-7e7c-4d56-8853-7246ca8dd01b",
   "metadata": {},
   "source": [
    "### Device & Random_seed setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02576ef-cf8d-474e-abc6-6e247b974191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe2557-0b1c-4e1f-be8f-7e50770e717d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c12fa-20de-4a59-b9f7-4c353a915ee2",
   "metadata": {},
   "source": [
    "### Kobert for Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26dbcaef-69d8-462a-a1ed-d208bcfcab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'skt/kobert-base-v1'\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1465c41-dc41-4cc7-a8ed-597a87bb950b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22f3728-ef98-48b8-93bc-c6fecdd3bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e8f1ba7701d4e902\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-e8f1ba7701d4e902/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0f049255274a588d8dba52edc21c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-21a9e1aedcee44a1\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-21a9e1aedcee44a1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1120bf0d4d345a784a9aaad4a257d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/piai/.cache/huggingface/datasets/csv/default-e8f1ba7701d4e902/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-f24d4418ea6a3ed7.arrow\n",
      "Loading cached processed dataset at /home/piai/.cache/huggingface/datasets/csv/default-21a9e1aedcee44a1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-37a56cd247a4152b.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "_metric = load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "def tokenized_fn(data):\n",
    "    outputs = tokenizer(data[\"article\"], padding=True, max_length=MAX_LEN, truncation=True)\n",
    "    if 'label' in data:\n",
    "        outputs[\"labels\"] = data[\"label\"]\n",
    "    return outputs\n",
    "\n",
    "def metric_fn(p):\n",
    "    preds, labels = p\n",
    "    output = _metric.compute(references=lables, predictions=np.argmax(preds, axis=-1))\n",
    "    return output\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"./data/train_report.csv\")[\"train\"]\n",
    "valid_dataset = load_dataset(\"csv\", data_files=\"./data/valid_report.csv\")[\"train\"]\n",
    "\n",
    "train_dataset = train_dataset.map(tokenized_fn, remove_columns=['filename', 'article', 'length', 'label'])\n",
    "valid_dataset = valid_dataset.map(tokenized_fn, remove_columns=['filename', 'article', 'length', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1d2c3a-92fc-48a4-a4b6-5704f8bc51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.dense1 = torch.nn.Linear(768, 3072)\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "        # self.dense2 = torch.nn.Linear(3072, 768)\n",
    "        self.output = torch.nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # 보통 분류기에선 start 토큰에 분류 결과를 담음\n",
    "        x = features[:, 0, :]    # take <s> token (equiv. to [CLS])\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "#         x = self.dense1(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.dense2(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class KobertForAinalyst(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(KobertForAinalyst, self).__init__()\n",
    "        self.model = model\n",
    "        self.classifier = ClassificationHead()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # labels=labels\n",
    "        )\n",
    "        self.labels = labels\n",
    "        logits = self.classifier(outputs[\"last_hidden_state\"])\n",
    "        # prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2114484e-d10e-413a-bfbc-cb33b69375ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.DataParallel(model)\n",
    "# isParallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c041aa-0c19-4e0a-95b6-489cee7b4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec99053-6d50-44c8-b69e-fed37c1f39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = torch.utils.data.RandomSampler(train_dataset),\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = _collator,\n",
    ")\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    sampler = torch.utils.data.SequentialSampler(valid_dataset),\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = _collator,\n",
    ")\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     'runs/',\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=30,\n",
    "#     save_total_limit=5,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     # eval_epochs=1,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     load_best_model_at_end = True,\n",
    "#     # metric_for_best_model = 'f1',\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         data_collator=_collator,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=valid_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         compute_metrics=metric_fn,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience = 5)]\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ae679b-f90e-4528-9008-78397add1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adda6a5d-f833-4522-846b-ac98d9193a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5195bc-d953-4b2f-8968-12b6054c0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 1/30 ====================\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/138 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25206/1018232799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         logits, loss = model(\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f\"==================== Epoch {epoch+1}/{epochs} ====================\")\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        logits, loss = model(\n",
    "            input_ids = batch_input_ids,\n",
    "            attention_mask = batch_attention_mask,\n",
    "            labels = batch_labels,\n",
    "        )\n",
    "        \n",
    "        # # output 모양 출력해보기\n",
    "        # outputs = model(\n",
    "        #     input_ids = batch_input_ids,\n",
    "        #     attention_mask = batch_attention_mask,\n",
    "        #     labels = batch_labels,\n",
    "        # )\n",
    "        # print(\"print outputs : \")\n",
    "        # print(outputs)\n",
    "        # print(outputs.last_hidden_state.shape)\n",
    "        # break\n",
    "    \n",
    "        if isParallel:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            print(\"step : {:>5,} of {:>5,} loss: {:.5f}\".format(step, len(train_dataloader), loss.item()))\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"    >>> Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "    \n",
    "    # Validation\n",
    "    print()\n",
    "    print(\"Running Validation...\")\n",
    "    \n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(valid_dataloader)):\n",
    "        batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(\n",
    "                input_ids = batch_input_ids,\n",
    "                attention_mask = batch_attention_mask,\n",
    "                labels = batch_labels,\n",
    "            )\n",
    "            \n",
    "            if isParallel:\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = batch_labels.to(\"cpu\").numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n",
    "    print(\"    >>> Accuracy: {0:.5f}\".format(avg_val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010186cc-6c03-4d51-bcde-c2f991c257e0",
   "metadata": {},
   "source": [
    "---\n",
    "#### Kobert by using kobert library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5357c5b4-c602-41ce-b470-853944958e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/piai/hustar/Hustar_Group_4_TeamP/testMH/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
      "/home/piai/hustar/Hustar_Group_4_TeamP/testMH/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "model, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f705d1-b6f4-4b45-abbb-dc1c1a481103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nlp.data.TSVDataset('./data/train_report.tsv', field_indices=[1,3], num_discard_samples=1)\n",
    "valid_dataset = nlp.data.TSVDataset('./data/valid_report.tsv', field_indices=[1,3], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89994ad0-753c-457c-a1fc-0ce703e28653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하지만 이번 경기 Cycle 에는 이자유예, 양극화 등 여러 요인으로 자산건전성이 경기에 후행할 수도 있다. 2021 년 순이익 추정치를 1 조 5,560 억원으로 6% 상향조정하지만, 여느 때에 비해 실적의 불확실성은 높은 편이다. 장기적인 배당 불확실성 감안하여 목표주가 9,000 원으로 하향조정기업은행은 국책은행이어서 금감원의 배당 축소 권고 대상에 포함되지 않는다. 그럼에도 불구하고 과연 배당성향을 더 높일 수 있을지는 불확실하다. 앞으로도 정책적인 역할을 중시할 가능성이 있기 때문이다. 장기 배당성향 기대치를 35%에서 30%로 낮추면서목표주가를 10,000 원에서 9,000 원으로 하향조정한다. 기존의 ‘중립’ 의견을 유지한다.',\n",
       " '0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f609ae8f-dffb-41dc-b394-9921e43fba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf201881-f41e-4c75-8ca1-ef02831bf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 30\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91cd97ed-5dc0-4342-b6a8-036197275f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #토큰화\n",
    "# tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)\n",
    "\n",
    "# #BERTDataset 클래스 이용, TensorDataset으로 만들어주기\n",
    "# train_dataset = BERTDataset(train_dataset, 0, 1, tokenizer, max_len, True, False)\n",
    "# valid_dataset = BERTDataset(valid_dataset, 0, 1, tokenizer, max_len, True, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
